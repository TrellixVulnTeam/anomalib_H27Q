{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.models.detection as detection\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from matplotlib import pyplot as plt\n",
    "from pytorch_lightning import Trainer\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.ops import RoIAlign\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "from anomalib.config import get_configurable_parameters\n",
    "from anomalib.data import InferenceDataset, get_datamodule\n",
    "from anomalib.models import get_model\n",
    "from anomalib.models.rkde.feature import FeatureExtractor as FeatureExtractor1\n",
    "from anomalib.models.rkde.feature_extractor import FeatureExtractor as FeatureExtractor2\n",
    "from anomalib.models.rkde.feature_extractor import RegionExtractor as RegionExtractor2\n",
    "from anomalib.models.rkde.region import RegionExtractor as RegionExtractor1\n",
    "from anomalib.models.rkde.torch_model import RkdeModel\n",
    "from anomalib.pre_processing.pre_process import PreProcessor, get_transforms\n",
    "from anomalib.utils.callbacks import LoadModelCallback, get_callbacks\n",
    "from anomalib.utils.loggers import configure_logger, get_experiment_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_configurable_parameters(config_path=\"./anomalib/models/rkde/ucsd.yaml\")\n",
    "\n",
    "datamodule = get_datamodule(config)\n",
    "model = get_model(config)\n",
    "experiment_logger = get_experiment_logger(config)\n",
    "callbacks = get_callbacks(config)\n",
    "\n",
    "trainer = Trainer(**config.trainer, logger=experiment_logger, callbacks=callbacks)\n",
    "trainer.fit(model=model, datamodule=datamodule)\n",
    "# TODO: Training returns an error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"anomalib/models/rkde/150.tif\"\n",
    "image = cv2.imread(filename)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transformations.\n",
    "transforms = get_transforms(config=\"anomalib/models/rkde/transforms.yaml\")\n",
    "pre_process = PreProcessor(config=transforms)\n",
    "\n",
    "# Get the data via dataloader\n",
    "dataset = InferenceDataset(path=\"anomalib/models/rkde/\", pre_process=pre_process)\n",
    "dataloader = DataLoader(dataset, batch_size=2)\n",
    "i, data = next(enumerate(dataloader))\n",
    "# datamodule = get_datamodule(config=config)\n",
    "# datamodule.setup()\n",
    "# i, data = next(enumerate(datamodule.test_dataloader()))\n",
    "\n",
    "# Create the region extractor.\n",
    "stage=\"rcnn\"\n",
    "use_original = False\n",
    "region_extractor1 = RegionExtractor1(stage=stage, use_original=use_original).eval().cuda()\n",
    "region_extractor2 = RegionExtractor2(stage=stage, use_original=use_original).eval().cuda()\n",
    "torch_model = RkdeModel(region_extractor_stage=stage).cuda()\n",
    "\n",
    "# Forward-Pass the input\n",
    "boxes1 = region_extractor1([image])\n",
    "boxes2 = region_extractor2(data[\"image\"].cuda())\n",
    "\n",
    "# Feature Extractor\n",
    "feature_extractor1 = FeatureExtractor1().eval().cuda()\n",
    "feature_extractor2 = FeatureExtractor2().eval().cuda()\n",
    "features1 = feature_extractor1(image, boxes1[0])\n",
    "features2 = feature_extractor2(data[\"image\"].cuda())\n",
    "features3 = torch_model.get_features(data[\"image\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.pad(input=boxes2, pad=(1, 0, 0, 0), mode=\"constant\", value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat(boxes2, dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_pt = torch.tensor(boxes1[0], dtype=torch.float32)\n",
    "rois = torch.cat((torch.zeros(boxes_pt.size(0), 1), boxes_pt), 1).unsqueeze(0)\n",
    "boxes_pt.shape, rois.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((torch.zeros(boxes_pt.size(0), 1), boxes_pt), 1).shape, boxes_pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('anomalib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae223df28f60859a2f400fae8b3a1034248e0a469f5599fd9a89c32908ed7a84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
