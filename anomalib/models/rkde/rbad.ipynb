{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from matplotlib import pyplot as plt\n",
    "from anomalib.pre_processing.pre_process import get_transforms, PreProcessor\n",
    "import torchvision.models.detection as detection\n",
    "from anomalib.data import InferenceDataset\n",
    "from torchvision.ops import RoIAlign\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from anomalib.models.rkde.feature_extractor import RegionExtractor as RegionExtractor2\n",
    "from anomalib.models.rkde.region import RegionExtractor as RegionExtractor1\n",
    "from anomalib.models.rkde.feature import FeatureExtractor as FeatureExtractor1\n",
    "from anomalib.models.rkde.feature_extractor import FeatureExtractor as FeatureExtractor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"150.tif\"\n",
    "image = cv2.imread(filename)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transformations.\n",
    "transforms = get_transforms(config=\"transforms.yaml\")\n",
    "pre_process = PreProcessor(config=transforms)\n",
    "\n",
    "# Get the data via dataloader\n",
    "dataset = InferenceDataset(path=filename, pre_process=pre_process)\n",
    "dataloader = DataLoader(dataset)\n",
    "i, data = next(enumerate(dataloader))\n",
    "\n",
    "# Create the region extractor.\n",
    "stage=\"rcnn\"\n",
    "use_original = False\n",
    "region_extractor1 = RegionExtractor1(stage=stage, use_original=use_original).eval().cuda()\n",
    "region_extractor2 = RegionExtractor2(stage=stage, use_original=use_original).eval().cuda()\n",
    "\n",
    "# Forward-Pass the input\n",
    "boxes1 = region_extractor1([image])\n",
    "boxes2 = region_extractor2(data[\"image\"].cuda())\n",
    "\n",
    "# Feature Extractor\n",
    "feature_extractor1 = FeatureExtractor1().eval().cuda()\n",
    "feature_extractor2 = FeatureExtractor2().eval().cuda()\n",
    "features1 = feature_extractor1(image, boxes1[0])\n",
    "features2 = feature_extractor2(data[\"image\"].cuda())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('anomalib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae223df28f60859a2f400fae8b3a1034248e0a469f5599fd9a89c32908ed7a84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
