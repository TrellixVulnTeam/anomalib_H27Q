{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from matplotlib import pyplot as plt\n",
    "from anomalib.pre_processing.pre_process import get_transforms\n",
    "import torchvision.models.detection as detection\n",
    "from anomalib.data import InferenceDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from anomalib.pre_processing import PreProcessor\n",
    "\n",
    "from anomalib.models.rbad.region_extractor import RegionExtractor as RegionExtractor2\n",
    "from anomalib.models.rbad.region import RegionExtractor as RegionExtractor1\n",
    "\n",
    "filename = \"150.tif\"\n",
    "image = cv2.imread(filename)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations.\n",
    "transforms = get_transforms(config=A.Compose([A.Normalize(mean=0.0, std=1.0), ToTensorV2()]))\n",
    "pre_process = PreProcessor(config=transforms)\n",
    "\n",
    "# Get the data via dataloader\n",
    "dataset = InferenceDataset(path=filename, pre_process=pre_process)\n",
    "dataloader = DataLoader(dataset)\n",
    "i, data = next(enumerate(dataloader))\n",
    "\n",
    "# Create the region extractor.\n",
    "stage=\"rpn\"\n",
    "use_original = True\n",
    "region_extractor1 = RegionExtractor1(stage=stage, use_original=use_original).eval().cuda()\n",
    "region_extractor2 = RegionExtractor2(stage=stage, use_original=use_original).eval().cuda()\n",
    "\n",
    "# Forward-Pass the input\n",
    "out1 = region_extractor1([image])\n",
    "out2 = region_extractor2(data[\"image\"].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 77.919365 ,  96.48182  ,  89.83704  , 128.9149   ],\n",
       "        [115.46756  ,  82.48316  , 126.88628  , 109.9148   ],\n",
       "        [203.38243  ,  48.828087 , 211.18283  ,  67.71943  ],\n",
       "        [166.53041  ,  37.683304 , 173.75429  ,  56.55531  ],\n",
       "        [135.94873  ,  42.22176  , 143.47353  ,  62.55404  ],\n",
       "        [128.62746  ,  84.20132  , 139.80513  , 107.595146 ],\n",
       "        [ 91.97258  ,  60.271667 , 101.65103  ,  81.26704  ],\n",
       "        [155.34999  ,  58.688763 , 163.23871  ,  81.62608  ],\n",
       "        [144.49686  ,  58.143917 , 153.59395  ,  80.6864   ],\n",
       "        [  1.6176828, 133.48872  ,  12.090294 , 158.       ],\n",
       "        [116.0117   ,  86.902084 , 122.413864 ,  95.00669  ],\n",
       "        [129.58817  ,  97.38628  , 135.42203  , 112.01805  ],\n",
       "        [129.29918  ,  96.54453  , 136.47055  , 112.37445  ],\n",
       "        [203.97433  ,  56.81103  , 209.38437  ,  67.685005 ],\n",
       "        [130.07632  ,  88.14842  , 135.91035  ,  94.70936  ],\n",
       "        [136.83748  ,  44.015156 , 142.04861  ,  51.518982 ],\n",
       "        [ 95.507324 ,  63.03795  , 101.08657  ,  69.67989  ],\n",
       "        [129.10057  , 101.25541  , 133.81142  , 112.509926 ],\n",
       "        [156.20654  ,  79.58657  , 160.99838  ,  81.72817  ],\n",
       "        [129.49542  , 101.55636  , 133.52222  , 112.26058  ],\n",
       "        [  0.7514133, 113.34126  ,  14.05725  , 152.82474  ],\n",
       "        [131.41649  ,  96.61853  , 136.51526  , 110.22931  ],\n",
       "        [117.7703   ,  86.587364 , 125.24816  ,  94.09068  ],\n",
       "        [204.16205  ,  57.242077 , 209.08965  ,  67.71342  ]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(out1[0], out2[0], rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2_images = [image]\n",
    "input_list = []\n",
    "for cv2_image in cv2_images:\n",
    "    cv2_image = cv2_image.astype(np.float32)\n",
    "    cv2_image /= 255.0\n",
    "    cv2_image = torch.from_numpy(cv2_image).contiguous()\n",
    "    cv2_image = cv2_image.permute(2, 0, 1)\n",
    "    input_list.append(cv2_image.cuda())\n",
    "\n",
    "original_image_sizes = [img.shape[-2:] for img in input_list]\n",
    "images, targets = region_extractor1.transform(input_list)\n",
    "new_image_sizes = images.image_sizes\n",
    "\n",
    "features = region_extractor1.backbone(images.tensors)\n",
    "if isinstance(features, torch.Tensor):\n",
    "    features = OrderedDict([(0, features)])\n",
    "\n",
    "proposals, proposal_losses = region_extractor1.rpn(images, features, targets)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = data[\"image\"].cuda()\n",
    "images = [image for image in input_tensor]\n",
    "\n",
    "original_images_sizes2 = [image.shape[-2:] for image in images]\n",
    "original_images_sizes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('anomalib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae223df28f60859a2f400fae8b3a1034248e0a469f5599fd9a89c32908ed7a84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
