{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from matplotlib import pyplot as plt\n",
    "from anomalib.pre_processing.pre_process import get_transforms, PreProcessor\n",
    "import torchvision.models.detection as detection\n",
    "from anomalib.data import InferenceDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from anomalib.models.rbad.region_extractor import RegionExtractor as RegionExtractor2\n",
    "from anomalib.models.rbad.region import RegionExtractor as RegionExtractor1\n",
    "from anomalib.models.rbad.feature import FeatureExtractor as FeatureExtractor1\n",
    "from anomalib.models.rbad.feature_extractor import FeatureExtractor as FeatureExtractor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"150.tif\"\n",
    "image = cv2.imread(filename)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transformations.\n",
    "transforms = get_transforms(config=A.Compose([A.Normalize(mean=0.0, std=1.0), ToTensorV2()]))\n",
    "pre_process = PreProcessor(config=transforms)\n",
    "\n",
    "# Get the data via dataloader\n",
    "dataset = InferenceDataset(path=filename, pre_process=pre_process)\n",
    "dataloader = DataLoader(dataset)\n",
    "i, data = next(enumerate(dataloader))\n",
    "\n",
    "# Create the region extractor.\n",
    "stage=\"rpn\"\n",
    "use_original = True\n",
    "region_extractor1 = RegionExtractor1(stage=stage, use_original=use_original).eval().cuda()\n",
    "region_extractor2 = RegionExtractor2(stage=stage, use_original=use_original).eval().cuda()\n",
    "\n",
    "# Forward-Pass the input\n",
    "boxes1 = region_extractor1([image])\n",
    "boxes2 = region_extractor2(data[\"image\"].cuda())\n",
    "\n",
    "# Feature Extractor\n",
    "feature_extractor1 = FeatureExtractor1().eval().cuda()\n",
    "feature_extractor2 = FeatureExtractor2().eval().cuda()\n",
    "features1 = feature_extractor1(image, boxes1[0])\n",
    "features2 = feature_extractor2(data[\"image\"].cuda(), boxes2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(filename)\n",
    "boxes = boxes1[0]\n",
    "image1, scale1 = feature_extractor1.transform(image)\n",
    "boxes_pt = torch.tensor(boxes)\n",
    "rois1 = torch.cat((torch.zeros(boxes_pt.size(0), 1), boxes_pt), 1).unsqueeze(0).to(feature_extractor1.device)\n",
    "rois1 *= scale1\n",
    "base_feats1 = feature_extractor1.head_module(image1)\n",
    "rcnn_feats1 = feature_extractor1.rcnn_module(base_feats1, rois1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image2, scale2 = feature_extractor2.transform(data[\"image\"].cuda())\n",
    "\n",
    "# Process RoIs.\n",
    "# Convert list of boxes to batch format.\n",
    "rois = [box.unsqueeze(0) for box in boxes]\n",
    "rois = torch.cat(boxes, dim=0)\n",
    "# Add zero column for the scores.\n",
    "rois = F.pad(input=rois, pad=(1, 0, 0, 0), mode=\"constant\", value=0)\n",
    "# Scale the RoIs based on the the new image size.\n",
    "rois *= scale\n",
    "\n",
    "# Extract RCNN features.\n",
    "backbone_features = self.backbone(images)\n",
    "rcnn_features = self.rcnn_module(backbone_features, rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.imread(filename)\n",
    "# # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# print(image.shape)\n",
    "\n",
    "# in_shape = image.shape\n",
    "# in_size_min = np.min(in_shape[0:2])\n",
    "# in_size_max = np.max(in_shape[0:2])\n",
    "\n",
    "# target_size = 600\n",
    "# max_size = 1000\n",
    "\n",
    "# scale = float(target_size) / float(in_size_min)\n",
    "# if np.round(scale * in_size_max) > max_size:\n",
    "#     print(\"WARNING: cfg.MAX_SIZE exceeded. Using a different scaling ratio\")\n",
    "#     scale = float(max_size) / float(in_size_max)\n",
    "\n",
    "# # TODO: Use the scale version.\n",
    "# image = cv2.resize(image, None, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "# image = cv2.resize(image, (904, 600), interpolation=cv2.INTER_LINEAR)\n",
    "# image = image.astype(np.float32)\n",
    "# image /= 255.0\n",
    "# image = torch.from_numpy(image)\n",
    "# image = image.unsqueeze(0)\n",
    "# image = image.permute(0, 3, 1, 2)\n",
    "# image = feature_extractor1.normalizer.normalize(image)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     image = image.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 600, 904])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 158, 238])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = data[\"image\"].cuda()\n",
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 5])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes = F.pad(input=boxes_pt, pad=(1, 0, 0, 0), mode=\"constant\", value=0).unsqueeze(dim=0)\n",
    "boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((torch.zeros(boxes_pt.size(0), 1), boxes_pt), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 5])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 4]), torch.Size([1, 24, 5]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes_pt = torch.tensor(boxes1[0])\n",
    "rois = torch.cat((torch.zeros(boxes_pt.size(0), 1), boxes_pt), 1).unsqueeze(0).to(input_tensor.device)\n",
    "boxes_pt.shape, rois.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply the feature extractor transforms \n",
    "# height, width = input_tensor.shape[2:]\n",
    "# shorter_image_size, longer_image_size = min(height, width), max(height, width)\n",
    "# target_image_size, max_image_size = 600, 1000\n",
    "\n",
    "# scale = target_image_size / shorter_image_size\n",
    "# if round(scale * longer_image_size) > max_image_size:\n",
    "#     print(\"WARNING: cfg.MAX_SIZE exceeded. Using a different scaling ratio\")\n",
    "#     scale = max_image_size / longer_image_size\n",
    "\n",
    "# # TODO: Use scale_factor\n",
    "# # resized_image = F.interpolate(input_tensor, scale_factor=scale, mode=\"bilinear\", align_corners=False)\n",
    "# resized_tensor = F.interpolate(input_tensor, size=(600, 904), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "# # Apply the same transformation as the original model.\n",
    "# mean = torch.tensor([0.485, 0.456, 0.406]).unsqueeze(0).unsqueeze(2).unsqueeze(3).to(input_tensor.device)\n",
    "# std = torch.tensor([0.229, 0.224, 0.225]).unsqueeze(0).unsqueeze(2).unsqueeze(3).to(input_tensor.device)\n",
    "# normalized_tensor = (resized_tensor - mean) / std\n",
    "\n",
    "# # NOTE: Return the normalized_tensor and scale here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.5938, -0.5938, -0.6109,  ..., -1.0219, -0.9877, -0.9877],\n",
       "           [-0.5938, -0.5938, -0.6109,  ..., -1.0219, -0.9877, -0.9877],\n",
       "           [-0.6623, -0.6623, -0.6623,  ..., -0.9877, -0.9534, -0.9534],\n",
       "           ...,\n",
       "           [-0.1314, -0.1314, -0.1314,  ...,  1.0331,  1.0673,  1.0673],\n",
       "           [-0.1314, -0.1314, -0.1314,  ...,  1.0502,  1.0844,  1.0844],\n",
       "           [-0.1314, -0.1314, -0.1314,  ...,  1.0502,  1.0844,  1.0844]],\n",
       " \n",
       "          [[-0.4776, -0.4776, -0.4951,  ..., -0.9153, -0.8803, -0.8803],\n",
       "           [-0.4776, -0.4776, -0.4951,  ..., -0.9153, -0.8803, -0.8803],\n",
       "           [-0.5476, -0.5476, -0.5476,  ..., -0.8803, -0.8452, -0.8452],\n",
       "           ...,\n",
       "           [-0.0049, -0.0049, -0.0049,  ...,  1.1856,  1.2206,  1.2206],\n",
       "           [-0.0049, -0.0049, -0.0049,  ...,  1.2031,  1.2381,  1.2381],\n",
       "           [-0.0049, -0.0049, -0.0049,  ...,  1.2031,  1.2381,  1.2381]],\n",
       " \n",
       "          [[-0.2532, -0.2532, -0.2707,  ..., -0.6890, -0.6541, -0.6541],\n",
       "           [-0.2532, -0.2532, -0.2707,  ..., -0.6890, -0.6541, -0.6541],\n",
       "           [-0.3230, -0.3230, -0.3230,  ..., -0.6541, -0.6193, -0.6193],\n",
       "           ...,\n",
       "           [ 0.2173,  0.2173,  0.2173,  ...,  1.4025,  1.4374,  1.4374],\n",
       "           [ 0.2173,  0.2173,  0.2173,  ...,  1.4200,  1.4548,  1.4548],\n",
       "           [ 0.2173,  0.2173,  0.2173,  ...,  1.4200,  1.4548,  1.4548]]]],\n",
       "        device='cuda:0'),\n",
       " 3.7974683544303796)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor1.transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-0.5938, -0.5938, -0.6019,  ..., -1.0310, -0.9877, -0.9877],\n",
       "           [-0.5938, -0.5938, -0.6019,  ..., -1.0310, -0.9877, -0.9877],\n",
       "           [-0.6616, -0.6616, -0.6689,  ..., -0.9999, -0.9497, -0.9497],\n",
       "           ...,\n",
       "           [-0.1341, -0.1341, -0.1327,  ...,  1.0198,  1.0763,  1.0763],\n",
       "           [-0.1314, -0.1314, -0.1287,  ...,  1.0275,  1.0844,  1.0844],\n",
       "           [-0.1314, -0.1314, -0.1287,  ...,  1.0275,  1.0844,  1.0844]],\n",
       " \n",
       "          [[-0.4776, -0.4776, -0.4859,  ..., -0.9246, -0.8803, -0.8803],\n",
       "           [-0.4776, -0.4776, -0.4859,  ..., -0.9246, -0.8803, -0.8803],\n",
       "           [-0.5469, -0.5469, -0.5543,  ..., -0.8928, -0.8414, -0.8414],\n",
       "           ...,\n",
       "           [-0.0077, -0.0077, -0.0062,  ...,  1.1721,  1.2298,  1.2298],\n",
       "           [-0.0049, -0.0049, -0.0021,  ...,  1.1799,  1.2381,  1.2381],\n",
       "           [-0.0049, -0.0049, -0.0021,  ...,  1.1799,  1.2381,  1.2381]],\n",
       " \n",
       "          [[-0.2532, -0.2532, -0.2615,  ..., -0.6982, -0.6541, -0.6541],\n",
       "           [-0.2532, -0.2532, -0.2615,  ..., -0.6982, -0.6541, -0.6541],\n",
       "           [-0.3222, -0.3222, -0.3296,  ..., -0.6666, -0.6155, -0.6155],\n",
       "           ...,\n",
       "           [ 0.2146,  0.2146,  0.2160,  ...,  1.3891,  1.4465,  1.4465],\n",
       "           [ 0.2173,  0.2173,  0.2201,  ...,  1.3969,  1.4548,  1.4548],\n",
       "           [ 0.2173,  0.2173,  0.2201,  ...,  1.3969,  1.4548,  1.4548]]]],\n",
       "        device='cuda:0'),\n",
       " 3.7974683544303796)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor2.transform(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 4096])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Old Forward-Pass\n",
    "image = cv2.imread(filename)\n",
    "image, scale = feature_extractor1.transform(image)\n",
    "boxes_pt = torch.tensor(boxes1[0])\n",
    "rois = torch.cat((torch.zeros(boxes_pt.size(0), 1), boxes_pt), 1).unsqueeze(0).to(feature_extractor1.device)\n",
    "rois *= scale\n",
    "\n",
    "base_feats = feature_extractor1.head_module(image)\n",
    "rcnn_feats = feature_extractor1.rcnn_module(base_feats, rois)\n",
    "\n",
    "rcnn_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward-Pass\n",
    "# TODO: Replace this with the new transforms.\n",
    "image = cv2.imread(filename)\n",
    "image, scale = feature_extractor1.transform(image)\n",
    "\n",
    "# TODO: Remove the following line\n",
    "boxes = [box.unsqueeze(0) for box in boxes2]\n",
    "boxes = torch.cat(boxes, dim=0)\n",
    "boxes = F.pad(input=boxes, pad=(1, 0, 0, 0), mode=\"constant\", value=0)\n",
    "boxes *= scale\n",
    "\n",
    "base_feats = feature_extractor1.head_module(image)\n",
    "rcnn_feats = feature_extractor1.rcnn_module(base_feats, boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 24, 5]), torch.Size([1, 24, 5]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rois.shape, boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('anomalib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae223df28f60859a2f400fae8b3a1034248e0a469f5599fd9a89c32908ed7a84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
